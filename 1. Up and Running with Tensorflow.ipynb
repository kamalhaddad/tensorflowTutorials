{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Up and Running with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to Install Tensorflow\n",
    "\n",
    "1. `pip install tensorflow`<br>If you have an Nvidia GPU: `pip install tensorflow-gpu`. <br>\n",
    "Just make sure you have your nvidia graphics driver 384, since it's required for CUDA 9.0, installed<br> `sudo apt-get install nvidia-384` .<br> Then download the CUDA run file<br>\n",
    "``wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda_9.0.176_384.81_linux-run``<br>\n",
    "**You can stop here if you don't have a GPU.**\n",
    "\n",
    "1. Run the run-file:<br>\n",
    "`chmod +x cuda_9.0.176_384.81_linux-run`<br>\n",
    "`./cuda_9.0.176_384.81_linux-run --extract=$HOME`\n",
    "\n",
    "1. You should have unpacked three components: NVIDIA-Linux-x86_64-384.81.run (1. NVIDIA driver that we ignore), cuda-linux.9.0.176-22781540.run (2. CUDA 9.0 installer), and cuda-samples.9.0.176-22781540-linux.run (3. CUDA 9.0 Samples).<br>\n",
    "Run the second one: `$ sudo ./cuda-linux.9.0.176-22781540.run`\n",
    "\n",
    "1. Add cuda to your environment variable `LD_LIBRARY_PATH`. I have it added in my .bashrc file and so it should look something like this: `LD_LIBRARY_PATH=/usr/local/cuda/lib64/:/usr/lib/nvidia-384`\n",
    "\n",
    "1. Install cuDNN 7.0: `sudo dpkg -i libcudnn7_7.0.5.15â€“1+cuda9.0_amd64.deb`\n",
    "\n",
    "Side note: To upgrade Tensorflow you can run `pip3 install --upgrade tensorflow-gpu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating your first graph and running it in a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.Variable(3, name='x')\n",
    "y = tf.Variable(4, name='y')\n",
    "f = x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code just creates the computation graph without executing them. We need to create a session for that and to initialze our variables. The session takes care of placing the operations onto devices such as CPUs or GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f)\n",
    "print(result)\n",
    "sess.close() #frees up resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating `sess.run()` is tedious. Thankfully, there's a better way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `x.initializer.run()` is equivalent to calling `tf.get_default_session().run(x.initializer)`. Our default session here is `sess` since we are inside the `sess` block. What's also nice is that our session closes automatically once the block as ended (smart pointers anybody)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of initializing every single variable, we can initialize all variables using `global_variables_initializer()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two types of sessions: Regular Sessions or Interactive Sessions. The difference is that one we create a regular session it doesn't set itself as the default session whereas the Interactive Session does. So we don't need a `with` block but we still need to close the session manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow unlike Pytorch for example has two phases: the Construction phase where we build our computation graph, and the Execution phase where we actually perform the computations usually required to train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any node we create is automatically added to the default graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to manage multiple independent graphs, well we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "    \n",
    "x2.graph is graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x2.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() #if we run the same command multiple times and re-add the nodes to the same graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow detects dependencies. Here for example, it first evaluates w, then x, then y. Then re-runs the graph to compute z. So twice.\n",
    "\n",
    "All node values are dropped between graph runs except variable values which are maintained by the session. A variable starts its life when its initializer is run and it ends when the session is closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval()) #10\n",
    "    print(z.eval()) #15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate y and z efficiently in one graph run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y,z])\n",
    "    print(y_val)\n",
    "    print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single process TF do not share any state even if they reuse the same graph (each session would have its own copy of every variable). In distributed TF, variable state is stored on the servers, not in the sessions, so multiple sessions can share the same variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression on Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow operations or ops: binary ops such as multiplication or additon, and source ops such as constants and variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Python API, tensors are actually numpy ndarrays (n-dimensional arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of applying gradient descent in the closed form using the Normal equation: $\\boldsymbol{\\hat\\theta} = {({X}^\\intercal X)}^{-1} X^\\intercal y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing() # fetching the housing dataset from sci-kit learn\n",
    "\n",
    "m,n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m,1)), housing.data] # adding the bias x0 using numpy column stack\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name='y')\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT,X)), XT), y) #setting the equation\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to apply batch gradient descent manually instead of the normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "standard_scalar = StandardScaler()\n",
    "\n",
    "scaled_housing_data_plus_bias = standard_scalar.fit_transform(housing_data_plus_bias) #don't forget to scale \n",
    "                                                                                      #your inputs before GD\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n+1,1], -1.0, 1.0), name='theta') #creates a node that will generate a tensor containing random values  given its shape and value range \n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error) # can be replaced with gradients = tf.gradients(mse,[theta])[0]\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients) #creates a node that will assign a new value to a variable\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch  in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval()) # Printing MSE every 100 iterations\n",
    "        sess.run(training_op)\n",
    "        \n",
    "    best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(mse)\n",
    "gradients = tf.gradients(mse, [theta])[0]\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods of Computing Gradients\n",
    "\n",
    "1. Manual Differentiation\n",
    "1. Symbolic Differentiation\n",
    "1. Numerical Differentiation\n",
    "1. Forward-Mode Autodiff\n",
    "1. Reverse-Mode Autodiff: Used by tensorflow and is optimal for high input low output functions just like NNs\n",
    "\n",
    "<a href=\"https://github.com/ageron/handson-ml/blob/master/extra_autodiff.ipynb\">Check out this link to understand more.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "Replace gradients and training_op with this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "#optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "training_op = optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding data dynamically\n",
    "\n",
    "We have mostly seen nodes with well-defined values at construction time and used using the execution phase. But what if we wanted to feed the training algorithm at run time. In mini-batch gradient descent we require to have a truly variable node with X's and y's being fed in batches. Thus, we use a placeholder node which will throw an exception at runtime if it wasn't filled. You can provide the shape to enforce that on the placeholder node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = tf.placeholder(tf.float32, shape=(None,3)) #enforcing A to be 2D with any number of rows and 3 columns\n",
    "B = A + 5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1,2,3]]}) # need to feed the dictionary\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4,5,6],[7,8,9]]}) # any output can be fed, not necessarily placeholders, it will use the values that were fed instead of evaluating the operations \n",
    "    \n",
    "print(B_val_1)\n",
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the unfinished code for mini-batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X = tf.placeholder(tf.float32, shape=(None,n+1), name='X')\n",
    "# y = tf.placeholder(tf.float32, shape=(None,1), name='y')\n",
    "\n",
    "# batch_size = 100\n",
    "# n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "# def fetch_batch(epoch, batch_index, batch_size):\n",
    "#     [...] # load data from disk\n",
    "#     return X_batch, y_batch\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "    \n",
    "#     for epoch in range(n_epochs):\n",
    "#         for batch_index in range(n_batches):\n",
    "#             X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "#             sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            \n",
    "#     best_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computers crash and we don't want to start over. Thankfully, Tensorflow has ways for us to save our models systematically during training so that if there's a crash during training, we can just reload the model and continue from there. It uses a `Saver` node that we can plug to the end of our graph and call save in the session whenever we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [...]\n",
    "# theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0))\n",
    "# [...]\n",
    "# init = tf.global_variables_initializer()\n",
    "# saver = tf.train.Saver() # adding a node at the end\n",
    "\n",
    "# with tf.Session as sess:\n",
    "#     sess.run(init)\n",
    "    \n",
    "#     for epoch in range(n_epochs):\n",
    "#         if epoch % 100 == 0:\n",
    "#             save_path = saver.save(sess, \"/tmp/my_model.ckpt\") #pass in the session and file path\n",
    "        \n",
    "#         sess.run(training_op)\n",
    "        \n",
    "#     best_theta = theta.eval()\n",
    "#     save_path = saver.save(sess, \"/tmp/my_moel_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading is similar, we have a `Saver` node at the end of the construction phase and call restore at the begininng of the execution phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     saver.restore(sess, \"/tmp/my_modell_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saver = tf.train.Saver({\"weights\":theta}) saving only theta under the name weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the `save()` method also saves the structure of the graph in a second `.meta` file. Loading that file adds the graph to the default graph structure using `tf.train.import_meta_graph()` and returns a `Saver` instance that can be used to restore the graph's state and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saver = tf.train.import_meta_graph(\"/tmp/my_model_final_ckpt.meta\")\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     saver.restore(sess, \"/tmp/my_model_final.ckpt\")\n",
    "#     [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization using TensorBoard\n",
    "\n",
    "To visualize the graph and training curves using TensorBoard, we need to tweak a couple of things.\n",
    "Add a log file and write MSE into it, but this log should be unique to the run or else tensorflow will merge multiple runs into the same log file and mess up the visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}\".format(root_logdir, now)\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "standard_scalar = StandardScaler()\n",
    "\n",
    "scaled_housing_data_plus_bias = standard_scalar.fit_transform(housing_data_plus_bias) #don't forget to scale \n",
    "                                                                                      #your inputs before GD\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n+1,1], -1.0, 1.0), name='theta') #creates a node that will generate a tensor containing random values  given its shape and value range \n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error) # can be replaced with gradients = tf.gradients(mse,[theta])[0]\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients) #creates a node that will assign a new value to a variable\n",
    "\n",
    "mse_summary = tf.summary.scalar('MSE', mse) # evaulates mse and dumps it into a binary log string called summary for tensorboard to read\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph()) # used to write summaries to binary log files called events file\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch  in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval()) # Printing MSE every 100 iterations\n",
    "        if epoch % 10 == 0:\n",
    "            summary_str = mse_summary.eval()\n",
    "            file_writer.add_summary(summary_str, epoch)\n",
    "        sess.run(training_op)\n",
    "        \n",
    "\n",
    "    best_theta = theta.eval()\n",
    "    file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize our graph and MSE using tensorboard. Run it using `tensorboard --logdir tf_logs/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define name scopes to reduce clutter on tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with tf.name_scope(\"loss\") as scope:\n",
    "#     error = y_pred - y\n",
    "#     mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "    \n",
    "# print(error.op.name) #loss/error\n",
    "# print(mse.op.name) #loss/mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modularity\n",
    "\n",
    "If we wanted to create a graph that adds the output of two rectified linear units(ReLU), writing each node individually would be tedious and require a lot of copying and pasting. Fortunately, Tensorflow lets you stay DRY (Don't Repeat Yourself) like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name='weights')\n",
    "    b = tf.Variable(0.0, name='bias')\n",
    "    z = tf.add(tf.matmul(X,w), b, name='z')\n",
    "    return tf.maximum(z, 0, name='relu')\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name='X')\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name='output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When Tensorflow creates a node, it checks whether its name already exists and if it does it appends an underscore followed by an index to make the name unique e.g: weights_1, bias_1, weights_2, bias_2... You can check them on TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using nam scopes, Tensorflow additionally gives nae scopes unique names by appending _1, _2..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def relu(X):\n",
    "#     with tf.name_scope(\"relu\"):\n",
    "#         [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sharing Variables\n",
    "\n",
    "If we wanted to share a variable between multiple components, like the threshold for our relu function, we could just pass it as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(X, threshold):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        [...]\n",
    "        return tf.maximum(z, threshold, 'max')\n",
    "\n",
    "threshold = tf.Variable(0.0, name='threshold')\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name='X')\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name='output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted to add more parameters, you could create a dictionary and pass it to the function, but it still seems like clutter. We could create a class called ReLU and have a member variable called threshold that you can use.<br> Yet, another option is to set the shared variable as an attribute of the relu() function upon the first call, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        if not hasattr(relu, \"threshold\"):\n",
    "            relu.threshold = tf.Variable(0.0, name='threshold')\n",
    "        [...]\n",
    "        return tf.maximum(z, relu.threshold, name='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow offers another option which is to use the `get_variable()` function to create the shared variable if it does not exist yet, or reuse it if it already exist and this is controlled by an attribute of the current `variable_scope()` not `name_scope()` which often confuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(), initializer=tf.constant_initializer(0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the variable was created previously then the above code will throw an exception. This behavior prevents reusing variables. To enable reusing variables, we must set the `reuse` attribute in the variable scope in which case we don't have to specify any shape or initializer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\", reuse=True):\n",
    "    threshold = tf.get_variable(\"threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code will fetch the existing \"**relu/threshold**\" variable, or raise an exception if it does not exist or if it was not created using `get_variable()`.<br>Alternatively, we can set the `reuse` attribute to True inside the block by calling the scope's `reuse_variables()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\") as scope:\n",
    "    scope.reuse_variables()\n",
    "    threshold = tf.get_variable(\"threshold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A complete view of how this will look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\", reuse=True):\n",
    "        threshold = tf.get_variable(\"threshold\") # getting the already existing reusable variable\n",
    "        [...]\n",
    "        return tf.maximum(z, threshold, name='max')\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name='X')\n",
    "with tf.variable_scope(\"relu\") : #create the variable\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(), initializer=tf.constant_initializer(0.0)) #shape=() means scalar\n",
    "    \n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name='output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables created using `get_variable()` are always named using the name of their `variable_scope` as a prefix (e.g \"**relu/threshold**\"), but for all other nodes such as `tf.Variable`'s, the variable scope acts like a new name scope. In particular, if a name scope with an identical name was already created, then a sufix is added to make the name unique. For example, all nodes we created before except threshold have names prefixed with `relu_1` to `relu_5`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't seem right that we have all our variables defined inside the `relu` function while the threshold is defined outside it. To fix this, the following code creates the threshold variable within the `relu()` function upon the first call, then reuses it in subsequent calls. Now the `relu()` function does not have to worry about name scopes or variable sharing: it just calls `get_variable()`, which will create or reuse the `threshold` variable and doesn't need to know which is the case. The rest of the code calls `relu()` five times, making sure to set `reuse=None` on the first call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(), initializer=tf.constant_intializer(0.0))\n",
    "    [...]\n",
    "    return tf.maximum(z ,threshold, name='max')\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name='X')\n",
    "relus=[]\n",
    "for relu_index in range(5):\n",
    "    with tf.variable_scope(\"relu\", reuse=(relu_index >= 1 or None)) as scope:\n",
    "        relus.append(relu(X))\n",
    "output = tf.add_n(relus, name='output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another thing that you will probably get mad at me for but since Tensorflow 1.4, you can set reuse=tf.AUTO_REUSE which returns the cariable if it already exists; otherwise, it creates it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This concludes the Up and Running with Tensorflow notebook, be on the look out for more advanced topics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
