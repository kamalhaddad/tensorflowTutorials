{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.Training Deep Neural Networks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "eAUBW5LjHszN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training Deep Neural Networks\n",
        "\n",
        "Three prominent problems that we face when trying to train deep neural networks are:\n",
        "\n",
        "1.   Vanishing/Exploding Gradients Problem\n",
        "2.   Slow training time\n",
        "3.  Risk of overfitting on the training set due to having a lot of parameters\n"
      ]
    },
    {
      "metadata": {
        "id": "vGypCdQdHxMT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Vanishing/Exploding Gradients Problem\n",
        "\n",
        "The backpropagation algorithm used to train neural networks uses gradient descent in order to do it and update the network's parameters with the proper gradients. Unfortunately, these gradients become smaller and smaller as we move towards the lower layers, making the layers virtually unchanged. This is called the *vanishing gradients* problem. \n",
        "\n",
        "The exploding gradients problem occurs when gradients grow bigger and bigger and thus diverge. This usually happens in recurrent neural networks.\n",
        "\n",
        "This instability is due to the combination of the popular logistic sigmoid activation function and the weight initialization technique using normal distribution with a mean of 0 and a standard deviation of 1, mainly that the variance of the output is greater than that of the input. Going forward in the network, the variance keeps increasing after each layer until the activation function saturates at the top layers making the gradients close to 0. This is actually mode worse by the fact that the logistic function has a mean of 0.5, not 0 (tanh has a mean of 0 and behaves slightly better than the sigmoid function).\n",
        "\n",
        "### Xavier and He Initialization\n",
        "\n",
        "To solve this problem, we need to make the input variance the same as the output variance. So that the signal doesn't die out or blow up in both the forward and backward directions. Easier said than done. This is only possible if the number of inputs is the same as the number of outputs, yet there is a compromise that has proven to work very well in practice: the connection of the weights must be initialized randomly from a normal distribution with a mean of 0 and standard deviation $\\sigma=\\sqrt{\\frac{2}{n_{inputs} + n_{outputs}}}$, where $n_{inputs}$ and $n_{outputs}$ are the number of input and output connections for the layer whose weights are being initialized (when using the sigmoid activation function) (or from a uniform distribution between $-r$ and $r$ such that $r = \\sqrt{\\frac{6}{n_{inputs} + n_{outputs}}}$). This is called the Xavier (or Glorot) intialization.\n",
        "\n",
        "Using the Xavier initialization strategy can speed up training considerably and it is oe of the tricks that led to the current success of Deep Learning. There are also initialization strategies when using different activation functions such as the ReLU activation function or the hyberbolic tangent function. For  ReLU, the intialization functions for uniform and normal distributions are as follows: $\\sigma=\\sqrt{2}\\sqrt{\\frac{2}{n_{inputs} + n_{outputs}}}$ and $r=\\sqrt{2}\\sqrt{\\frac{6}{n_{inputs} + n_{outputs}}}$. This is called the He initializaton."
      ]
    },
    {
      "metadata": {
        "id": "AtTUlOaVjJMv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "By default, `tf.layers.dense()` function uses Xavier initialization with a uniform distribution, You can change this to a He initialization by using the `variance_scaling_intializer()` function like this:"
      ]
    },
    {
      "metadata": {
        "id": "mkheChUdt9wS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# To support both python 2 and python 3\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "def reset_graph(seed=42):\n",
        "    tf.reset_default_graph()\n",
        "    tf.set_random_seed(seed)\n",
        "    np.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dfaJwty4GQra",
        "colab_type": "code",
        "outputId": "44aa7230-0157-4f36-f7ef-4b397576bdec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "\n",
        "he_init = tf.variance_scaling_initializer()\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
        "                          kernel_initializer=he_init, name=\"hidden1\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-aa84b0e09f9f>:10: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BVlWSxeNjCjt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Nonsaturating activation functions\n",
        "\n",
        "Neurologists found that neurons use sigmoidal activation functions so they were considered as the first choice when building neural networks but it turns out that there are better activation functions such as ReLU activation mostly because it does not saturate for positive values, and because it fast to compute.\n",
        "\n",
        "Unfortunately, the ReLU activation function is not perfect as it suffers from a problem known as the dying ReLUs. During training, some neurons effectively ie, meaning that they stop outputting anything other than 0. In some cases, you might find half your neurons dead especially if you use a high learning rate. During training, if a neuron's weights get updated such that the weighted sum of the neuron's inputs is negative, it will start outputting 0. When this happens, the neuron is unlikely to come back to life since the gradient of the ReLU function is 0 when the input is negative.\n",
        "\n",
        "We can use variants of ReLU such as leaky ReLU where $LeakyReLU_{\\alpha}(z) = max(\\alpha z, z), where\\ \\alpha\\ is\\ the\\ slope\\ of\\ the\\ function\\ for\\ z<0$. We could also have $\\alpha$ be parametrized leading to a parametric leaky ReLU where it is authorized to be learned during training instead of it being set as a hyperparameter. It becomes a parameter that can be set by backpropagation.\n",
        "\n",
        "\n",
        "The ELU or exponential linear unit out-performed all ReLU variants in their experiments: training time was reduced and the neural network performed better on the test set. $ELU_{\\alpha} = \\begin{cases} \\alpha(e^{z}-1) & if\\ z < 0 \\\\ z & if\\ z \\geq 0   \\end{cases}$. The average output is now closer to 0 which helps alleviate the vanishing gradients problem and it has non-zero gradient when z is less than 0 which avoids the dying units issue, and the function is smooth everywhere which speeds up Gradient Descent. It is just slower to compute than ReLU and its variants due to the exponential function but this is compensated for by the faster convergence rate, but it will still be slower during test time."
      ]
    },
    {
      "metadata": {
        "id": "TkPJJXturG0t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here's an example of using ELU in TensorFlow:"
      ]
    },
    {
      "metadata": {
        "id": "lt8tf6hRrGTa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vgCkQSMXrUr4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name=\"hidden1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oNIDR6nErWmE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##SeLU\n",
        "This activation function was proposed in this great paper by Günter Klambauer, Thomas Unterthiner and Andreas Mayr, published in June 2017. During training, a neural network composed exclusively of a stack of dense layers using the SELU activation function and LeCun initialization will self-normalize: the output of each layer will tend to preserve the same mean and variance during training, which solves the vanishing/exploding gradients problem. As a result, this activation function outperforms the other activation functions very significantly for such neural nets, so you should really try it out. Unfortunately, the self-normalizing property of the SELU activation function is easily broken: you cannot use ℓ1 or ℓ2 regularization, regular dropout, max-norm, skip connections or other non-sequential topologies (so recurrent neural networks won't self-normalize). However, in practice it works quite well with sequential CNNs. If you break self-normalization, SELU will not necessarily outperform other activation functions."
      ]
    },
    {
      "metadata": {
        "id": "WjUvpWmUsf5p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Batch Normalization\n",
        "\n",
        "Although using the He initialization with ELU or ReLU variants can significantly reduce the vanishing/exploding gradients problems at the beginning, it does not guarantee that they won't come back during training. Batch Normalization is here to address those issues and more generally the problem that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change (which is called the *Internal Covariate Shift* problem).\n",
        "\n",
        "The technique consists of adding an operation in the model before the activation function of each layer, simply zero-centering and normalizing the inputs, then scaling and shifting the result using two new parameters per layer(one for scaling, the other for shifting). In other words, this operation lets the model learn the optimal scale and mean of the inputs for each layer. To zero-center and normalize the inputs, the algorithm needs to estimate the inputs' mean and standard deviation. It does so by evaluating the mean and standard deviation of the inputs over the current mini-batch or batch when testing not training so it will computer the training set's mean and standard deviation.\n",
        "\n",
        "So, in total, four parameters are learned for each batch-normalized layer: $\\gamma(scale), \\beta(offset), \\mu (mean), and\\ \\sigma(standard\\ deviation)$.\n",
        "\n",
        "This technique considerably improved deep neural networks. The vanishing gradients problem is strongly reduced, to the point that they could use saturating sactivation functions. The networks were also much less sensitive to the weight initialization. It also allows the usage of large learning rates. significantly speeding up the learning process. Finally, batch normalization also acts like a regulizer, reducing the need for other regularization techniques. \n",
        "\n",
        "However, it does add some complexity to the model such that it makes slower predictions due to the extra computations required at each layer. So, if you need predictions to be lightning fast, you may want to check how well plain ELU + He initialization perform before playing with batch normalization."
      ]
    },
    {
      "metadata": {
        "id": "dCuxDCJCe5my",
        "colab_type": "code",
        "outputId": "d8cd963c-449a-4475-9017-534942acaf53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "n_inputs = 28 * 28\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 100\n",
        "n_outputs = 10\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
        "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
        "bn1_act = tf.nn.elu(bn1)\n",
        "\n",
        "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
        "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
        "bn2_act = tf.nn.elu(bn2)\n",
        "\n",
        "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
        "logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-3c49c3b7a95e>:15: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.batch_normalization instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "opWZ6Bh-mYzl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Notice that we didn't set an activation function for the fully connected layers since we want to add an activation function right after batch normalization. The `training` placeholder is set to False first by default but will be set to True during training. It is used to tell whether we'll be using the mini-batch's mean and standard deviation or the training set's mean and standard deviation. During training, it should be set to `True`"
      ]
    },
    {
      "metadata": {
        "id": "ECsByYFMnesp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The batch normalization algorithm uses *exponential decay* to compute the running averages, which is why it requires the `momentum` parameter. $\\hat{v} \\leftarrow \\hat{v} \\times momentum + v(1-momentum)$. A good `momentum` value is typically close to 1 like 0.9, 0.99, 0.999 (you want more 9's for larger datasets and smaller mini-batches). "
      ]
    },
    {
      "metadata": {
        "id": "4SSdq0b9ss4o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To avoid repeating the same parameters over and over again, we can use Python's partial() function:\n"
      ]
    },
    {
      "metadata": {
        "id": "fzfCHh_csDxe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "reset_graph()\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "\n",
        "my_batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
        "\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
        "bn1 = my_batch_norm_layer(hidden1)\n",
        "bn1_act = tf.nn.elu(bn1)\n",
        "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
        "bn2 = my_batch_norm_layer(hidden2)\n",
        "bn2_act = tf.nn.elu(bn2)\n",
        "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
        "logits = my_batch_norm_layer(logits_before_bn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LPobo_REsxMM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's build a neural net for MNIST, using the ELU activation function and Batch Normalization at each layer:\n"
      ]
    },
    {
      "metadata": {
        "id": "lou7OzfbuO5e",
        "colab_type": "code",
        "outputId": "9bc991f1-14fe-4300-a906-c4bcf346b821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "batch_norm_momentum = 0.9\n",
        "learning_rate = 0.01\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    he_init = tf.variance_scaling_initializer()\n",
        "\n",
        "    my_batch_norm_layer = partial(\n",
        "            tf.layers.batch_normalization,\n",
        "            training=training,\n",
        "            momentum=batch_norm_momentum)\n",
        "\n",
        "    my_dense_layer = partial(\n",
        "            tf.layers.dense,\n",
        "            kernel_initializer=he_init)\n",
        "\n",
        "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
        "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
        "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
        "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
        "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
        "    logits = my_batch_norm_layer(logits_before_bn)\n",
        "\n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)\n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "    \n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NRmz6zL8uRtP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "When is_training is True the moving_mean and moving_variance need to be updated, by default the update_ops are placed in `tf.GraphKeys.UPDATE_OPS` so they need to be added as a dependency to the train_op. TensorFlow doesn't distinguish between training and testing phases as well as between graphs."
      ]
    },
    {
      "metadata": {
        "id": "f6S5cJW5uhVh",
        "colab_type": "code",
        "outputId": "a0d2f41a-45e1-41b0-f70a-1be91dd16603",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "cell_type": "code",
      "source": [
        "def shuffle_batch(X, y, batch_size):\n",
        "    rnd_idx = np.random.permutation(len(X))\n",
        "    n_batches = len(X) // batch_size\n",
        "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
        "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
        "        yield X_batch, y_batch\n",
        "        \n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
        "y_train = y_train.astype(np.int32)\n",
        "y_test = y_test.astype(np.int32)\n",
        "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
        "y_valid, y_train = y_train[:5000], y_train[5000:]\n",
        "        \n",
        "n_epochs = 20\n",
        "batch_size = 200\n",
        "\n",
        "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run([training_op, extra_update_ops], feed_dict={training: True, X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "0 Validation accuracy: 0.8952\n",
            "1 Validation accuracy: 0.9202\n",
            "2 Validation accuracy: 0.9318\n",
            "3 Validation accuracy: 0.9422\n",
            "4 Validation accuracy: 0.9468\n",
            "5 Validation accuracy: 0.954\n",
            "6 Validation accuracy: 0.9568\n",
            "7 Validation accuracy: 0.96\n",
            "8 Validation accuracy: 0.962\n",
            "9 Validation accuracy: 0.9638\n",
            "10 Validation accuracy: 0.9662\n",
            "11 Validation accuracy: 0.9682\n",
            "12 Validation accuracy: 0.9672\n",
            "13 Validation accuracy: 0.9696\n",
            "14 Validation accuracy: 0.9706\n",
            "15 Validation accuracy: 0.9704\n",
            "16 Validation accuracy: 0.9718\n",
            "17 Validation accuracy: 0.9726\n",
            "18 Validation accuracy: 0.9738\n",
            "19 Validation accuracy: 0.9742\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pHpMIzZ2uoHl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What!? That's not a great accuracy for MNIST,- about 97%. Of course, if you train for longer it will get much better accuracy, but with such a shallow network, Batch Norm and ELU are unlikely to have very positive impact: they shine mostly for much deeper nets.\n",
        "\n",
        "Note that you could also make the training operation depend on the update operations:"
      ]
    },
    {
      "metadata": {
        "id": "vutpvjEUxeoJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(extra_update_ops):\n",
        "        training_op = optimizer.minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8QRHPQ-Kxm7p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Note that you could also make the training operation depend on the update operations:"
      ]
    },
    {
      "metadata": {
        "id": "kUoY1igbxmef",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(extra_update_ops):\n",
        "        training_op = optimizer.minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W5VJEgFexts-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This way, you would just have to evaluate the training_op during training, TensorFlow would automatically run the update operations as well:"
      ]
    },
    {
      "metadata": {
        "id": "A67k76yXxygm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#   sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vc_bN2ifyIIt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "One more thing: notice that the list of trainable variables is shorter than the list of all global variables. This is because the moving averages are non-trainable variables. If you want to reuse a pretrained neural network (see below), you must not forget these non-trainable variables."
      ]
    },
    {
      "metadata": {
        "id": "MQoEQt1lyIaS",
        "colab_type": "code",
        "outputId": "ad7c3f68-ef41-4085-e3fa-23db057d4465",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "cell_type": "code",
      "source": [
        "[v.name for v in tf.trainable_variables()]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hidden1/kernel:0',\n",
              " 'hidden1/bias:0',\n",
              " 'batch_normalization/gamma:0',\n",
              " 'batch_normalization/beta:0',\n",
              " 'hidden2/kernel:0',\n",
              " 'hidden2/bias:0',\n",
              " 'batch_normalization_1/gamma:0',\n",
              " 'batch_normalization_1/beta:0',\n",
              " 'outputs/kernel:0',\n",
              " 'outputs/bias:0',\n",
              " 'batch_normalization_2/gamma:0',\n",
              " 'batch_normalization_2/beta:0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "fTHmlWCNyLO_",
        "colab_type": "code",
        "outputId": "60f58d55-99c5-4260-d335-d2219d1db679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "cell_type": "code",
      "source": [
        "[v.name for v in tf.global_variables()]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hidden1/kernel:0',\n",
              " 'hidden1/bias:0',\n",
              " 'batch_normalization/gamma:0',\n",
              " 'batch_normalization/beta:0',\n",
              " 'batch_normalization/moving_mean:0',\n",
              " 'batch_normalization/moving_variance:0',\n",
              " 'hidden2/kernel:0',\n",
              " 'hidden2/bias:0',\n",
              " 'batch_normalization_1/gamma:0',\n",
              " 'batch_normalization_1/beta:0',\n",
              " 'batch_normalization_1/moving_mean:0',\n",
              " 'batch_normalization_1/moving_variance:0',\n",
              " 'outputs/kernel:0',\n",
              " 'outputs/bias:0',\n",
              " 'batch_normalization_2/gamma:0',\n",
              " 'batch_normalization_2/beta:0',\n",
              " 'batch_normalization_2/moving_mean:0',\n",
              " 'batch_normalization_2/moving_variance:0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "W9fQx9CUyONp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Gradient Clipping\n",
        "\n",
        "A popular technique to lessen the exploding gradients problem is to simply clip the gradients during backpropagation so that they never exceed some threshold which is mostly useful for recurrent neural networks. This is Gradient Clipping. Nowadays, batch normalization is preferred but it's still useful to know about Gradient Clipping and how to implement it."
      ]
    },
    {
      "metadata": {
        "id": "NNjjxzUDyPzK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In TensorFlow, the optimizer's `minimize()` function takes care of both computing the gradients and applying them , so you must instead call the optimizer's `compute_gradients()` method furst then create an operation to clip the gradients using the `clip_by_value()` function, and finally create an operation to apply the clipped gradients using the optimizer's `apply_gradients()`.\n",
        "\n",
        "Let's create a simple neural net for MNIST and add gradient clipping. The first part is the same as earlier (except we added a few more layers to demonstrate reusing pretrained models, see below):"
      ]
    },
    {
      "metadata": {
        "id": "CuvaEuCqvPr8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 50\n",
        "n_hidden3 = 50\n",
        "n_hidden4 = 50\n",
        "n_hidden5 = 50\n",
        "n_outputs = 10\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
        "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
        "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
        "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
        "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
        "\n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xqVROAuMvY3l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "threshold = 1.0\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "grads_and_vars = optimizer.compute_gradients(loss)\n",
        "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\n",
        "training_op = optimizer.apply_gradients(capped_gvs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H9zw5u80vopx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The rest is the same as usual:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "UBGhKi_Mv8Lu",
        "colab_type": "code",
        "outputId": "09891af8-6934-41e6-e403-445472cabad2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
        "    \n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "n_epochs = 20\n",
        "batch_size = 200\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Validation accuracy: 0.2876\n",
            "1 Validation accuracy: 0.7944\n",
            "2 Validation accuracy: 0.8794\n",
            "3 Validation accuracy: 0.906\n",
            "4 Validation accuracy: 0.9162\n",
            "5 Validation accuracy: 0.9218\n",
            "6 Validation accuracy: 0.9292\n",
            "7 Validation accuracy: 0.9358\n",
            "8 Validation accuracy: 0.9378\n",
            "9 Validation accuracy: 0.9416\n",
            "10 Validation accuracy: 0.9454\n",
            "11 Validation accuracy: 0.947\n",
            "12 Validation accuracy: 0.9476\n",
            "13 Validation accuracy: 0.953\n",
            "14 Validation accuracy: 0.9564\n",
            "15 Validation accuracy: 0.9566\n",
            "16 Validation accuracy: 0.9574\n",
            "17 Validation accuracy: 0.9588\n",
            "18 Validation accuracy: 0.9624\n",
            "19 Validation accuracy: 0.961\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UswiJpXBwI8Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Reusing Pretrained Layers\n",
        "\n",
        "*Transfer Learning* is finding an existing neural network that accomplishes a similar task to the one you are trying to tackle, then reusing the lower layers of this network because it is generally not a good idea to train a very large DNN. It will not only speed up training considerably, but will also require mushc less training data.\n",
        "\n",
        "For example, suppose that you have access to a DNN that was trained to classify pictures into 100 different categories, including animals, plants, vehicles, and everyday objects. You now want to train a DNN to classify specific types of vehicles. Theses tasks are very similar, so you should try to reuse parts of the first network.\n",
        "\n",
        "If the input images of your new task don't have the same size as the ones used in the original task, you will have to add a pre-processing step to resize them to the size expected by the original model. More generally, transfer learning will only work well if inputs have similar low-level features."
      ]
    },
    {
      "metadata": {
        "id": "WmlOEJ5O63jm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Reusing a TensorFlow Model\n",
        "\n",
        "First you need to load the graph's structure. The import_meta_graph() function does just that, loading the graph's operations into the default graph, and returning a Saver that you can then use to restore the model's state. Note that by default, a Saver saves the structure of the graph into a .meta file, so that's the file you should load:"
      ]
    },
    {
      "metadata": {
        "id": "zFS67_Dj7Jk6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# reset_graph()\n",
        "# saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")\n",
        "# for op in tf.get_default_graph().get_operations():\n",
        "#     print(op.name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kzC-wTGK9RXZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Once you know which operations you need, you can get a handle on them using the graph's get_operation_by_name() or get_tensor_by_name() methods:"
      ]
    },
    {
      "metadata": {
        "id": "MsmaWlpU7Rxk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
        "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
        "\n",
        "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
        "\n",
        "training_op = tf.get_default_graph().get_operation_by_name(\"GradientDescent\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dyab60S5-dY1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "If you are the author of the original model, you could make things easier for people who will reuse your model by giving operations very clear names and documenting them. Another approach is to create a collection containing all the important operations that people will want to get a handle on:"
      ]
    },
    {
      "metadata": {
        "id": "M38rAPoC-dw0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for op in (X, y, accuracy, training_op):\n",
        "    tf.add_to_collection(\"my_important_ops\", op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "91so34K4-g_S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This way people who reuse your model will be able to simply write:\n"
      ]
    },
    {
      "metadata": {
        "id": "OW_eKRC_-jHd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X, y, accuracy, training_op = tf.get_collection(\"my_important_ops\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dEJHBTRa-qij",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now you can start a session, restore the model's state and continue training on your data:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "-0gwkzKP-rNq",
        "colab_type": "code",
        "outputId": "783c1a77-56d4-4289-ec2a-7d6243fda0d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
        "    # continue training the model..."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ooj52nhV-uih",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Actually, let's test this for real."
      ]
    },
    {
      "metadata": {
        "id": "d4HZVTLr-4Mt",
        "colab_type": "code",
        "outputId": "e9e54674-1aae-44b1-9da6-78a4a16c4086",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
            "0 Validation accuracy: 0.9634\n",
            "1 Validation accuracy: 0.9648\n",
            "2 Validation accuracy: 0.9646\n",
            "3 Validation accuracy: 0.9644\n",
            "4 Validation accuracy: 0.9666\n",
            "5 Validation accuracy: 0.9674\n",
            "6 Validation accuracy: 0.968\n",
            "7 Validation accuracy: 0.9664\n",
            "8 Validation accuracy: 0.966\n",
            "9 Validation accuracy: 0.971\n",
            "10 Validation accuracy: 0.9674\n",
            "11 Validation accuracy: 0.9702\n",
            "12 Validation accuracy: 0.9712\n",
            "13 Validation accuracy: 0.9706\n",
            "14 Validation accuracy: 0.9694\n",
            "15 Validation accuracy: 0.9698\n",
            "16 Validation accuracy: 0.9712\n",
            "17 Validation accuracy: 0.9692\n",
            "18 Validation accuracy: 0.9726\n",
            "19 Validation accuracy: 0.9712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f5BueIyl_CdW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Alternatively, if you have access to the Python code that built the original graph, you can use it instead of import_meta_graph():\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "FNT2ngzw_D2F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 50\n",
        "n_hidden3 = 50\n",
        "n_hidden4 = 50\n",
        "n_hidden5 = 50\n",
        "n_outputs = 10\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
        "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
        "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
        "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
        "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
        "\n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
        "\n",
        "learning_rate = 0.01\n",
        "threshold = 1.0\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "grads_and_vars = optimizer.compute_gradients(loss)\n",
        "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
        "              for grad, var in grads_and_vars]\n",
        "training_op = optimizer.apply_gradients(capped_gvs)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nN1jeu0m_JWb",
        "colab_type": "code",
        "outputId": "b13a691c-41c5-4395-84d8-a4ededbfb5b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
            "0 Validation accuracy: 0.964\n",
            "1 Validation accuracy: 0.9628\n",
            "2 Validation accuracy: 0.9652\n",
            "3 Validation accuracy: 0.9652\n",
            "4 Validation accuracy: 0.9644\n",
            "5 Validation accuracy: 0.9648\n",
            "6 Validation accuracy: 0.9686\n",
            "7 Validation accuracy: 0.9686\n",
            "8 Validation accuracy: 0.9684\n",
            "9 Validation accuracy: 0.9682\n",
            "10 Validation accuracy: 0.97\n",
            "11 Validation accuracy: 0.9714\n",
            "12 Validation accuracy: 0.9672\n",
            "13 Validation accuracy: 0.9698\n",
            "14 Validation accuracy: 0.9708\n",
            "15 Validation accuracy: 0.972\n",
            "16 Validation accuracy: 0.972\n",
            "17 Validation accuracy: 0.9712\n",
            "18 Validation accuracy: 0.9714\n",
            "19 Validation accuracy: 0.9712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oZpM6O_M_UIs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In general you will want to reuse only the lower layers. If you are using import_meta_graph() it will load the whole graph, but you can simply ignore the parts you do not need. In this example, we add a new 4th hidden layer on top of the pretrained 3rd layer (ignoring the old 4th hidden layer). We also build a new output layer, the loss for this new output, and a new optimizer to minimize it. We also need another saver to save the whole graph (containing both the entire old graph plus the new operations), and an initialization operation to initialize all the new variables:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "fi3wlAsl_Nfo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "n_hidden4 = 20  # new layer\n",
        "n_outputs = 10  # new layer\n",
        "\n",
        "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")\n",
        "\n",
        "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
        "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
        "\n",
        "hidden3 = tf.get_default_graph().get_tensor_by_name(\"dnn/hidden3/Relu:0\")\n",
        "\n",
        "new_hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"new_hidden4\")\n",
        "new_logits = tf.layers.dense(new_hidden4, n_outputs, name=\"new_outputs\")\n",
        "\n",
        "with tf.name_scope(\"new_loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "with tf.name_scope(\"new_eval\"):\n",
        "    correct = tf.nn.in_top_k(new_logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
        "\n",
        "with tf.name_scope(\"new_train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "new_saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VfWmauhoAY07",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And we can train this new model:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ZNxH7sIYAWrb",
        "colab_type": "code",
        "outputId": "7f47134a-3c80-4138-b911-3df2f03a1053",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
        "\n",
        "    save_path = new_saver.save(sess, \"./my_new_model_final.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
            "0 Validation accuracy: 0.919\n",
            "1 Validation accuracy: 0.9396\n",
            "2 Validation accuracy: 0.949\n",
            "3 Validation accuracy: 0.9528\n",
            "4 Validation accuracy: 0.9554\n",
            "5 Validation accuracy: 0.9556\n",
            "6 Validation accuracy: 0.9574\n",
            "7 Validation accuracy: 0.961\n",
            "8 Validation accuracy: 0.9612\n",
            "9 Validation accuracy: 0.9642\n",
            "10 Validation accuracy: 0.9648\n",
            "11 Validation accuracy: 0.966\n",
            "12 Validation accuracy: 0.9642\n",
            "13 Validation accuracy: 0.9672\n",
            "14 Validation accuracy: 0.9684\n",
            "15 Validation accuracy: 0.968\n",
            "16 Validation accuracy: 0.97\n",
            "17 Validation accuracy: 0.9676\n",
            "18 Validation accuracy: 0.9696\n",
            "19 Validation accuracy: 0.9704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vEG_Oc52AmoD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you have access to the Python code that built the original graph, you can just reuse the parts you need and drop the rest:\n"
      ]
    },
    {
      "metadata": {
        "id": "BXxEUM0aAguL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300 # reused\n",
        "n_hidden2 = 50  # reused\n",
        "n_hidden3 = 50  # reused\n",
        "n_hidden4 = 20  # new!\n",
        "n_outputs = 10  # new!\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n",
        "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n",
        "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n",
        "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n",
        "\n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qWuGKU-IAtFu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "However, you must create one Saver to restore the pretrained model (giving it the list of variables to restore, or else it will complain that the graphs don't match), and another Saver to save the new model, once it is trained:\n",
        "(Think about this in the context of a graph)"
      ]
    },
    {
      "metadata": {
        "id": "MiAzS1lyArQN",
        "colab_type": "code",
        "outputId": "7a6ed9c8-3fb6-4db9-bd52-c8f7abdccba4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "cell_type": "code",
      "source": [
        "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden[123]\") # regular expression\n",
        "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
        "\n",
        "    for epoch in range(n_epochs):                                            \n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): \n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})        \n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})     \n",
        "        print(epoch, \"Validation accuracy:\", accuracy_val)                   \n",
        "\n",
        "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
            "0 Validation accuracy: 0.9028\n",
            "1 Validation accuracy: 0.9332\n",
            "2 Validation accuracy: 0.9428\n",
            "3 Validation accuracy: 0.947\n",
            "4 Validation accuracy: 0.9518\n",
            "5 Validation accuracy: 0.9536\n",
            "6 Validation accuracy: 0.9558\n",
            "7 Validation accuracy: 0.9588\n",
            "8 Validation accuracy: 0.9588\n",
            "9 Validation accuracy: 0.9608\n",
            "10 Validation accuracy: 0.962\n",
            "11 Validation accuracy: 0.9618\n",
            "12 Validation accuracy: 0.964\n",
            "13 Validation accuracy: 0.9662\n",
            "14 Validation accuracy: 0.9662\n",
            "15 Validation accuracy: 0.9664\n",
            "16 Validation accuracy: 0.9672\n",
            "17 Validation accuracy: 0.9674\n",
            "18 Validation accuracy: 0.9682\n",
            "19 Validation accuracy: 0.9674\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OZ_JDdk0A07v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Reusing Models from Other Frameworks\n",
        "\n",
        "\n",
        "In this example, for each variable we want to reuse, we find its initializer's assignment operation, and we get its second input, which corresponds to the initialization value. When we run the initializer, we replace the initialization values with the ones we want, using a feed_dict:"
      ]
    },
    {
      "metadata": {
        "id": "I00NORJAGLIO",
        "colab_type": "code",
        "outputId": "1b25ea1c-0b07-4421-dac4-c4dcb0d4b862",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 2\n",
        "n_hidden1 = 3\n",
        "\n",
        "original_w = [[1., 2., 3.], [4., 5., 6.]] # Load the weights from the other framework\n",
        "original_b = [7., 8., 9.]                 # Load the biases from the other framework\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
        "# [...] Build the rest of the model\n",
        "\n",
        "# Get a handle on the assignment nodes for the hidden1 variables\n",
        "graph = tf.get_default_graph()\n",
        "assign_kernel = graph.get_operation_by_name(\"hidden1/kernel/Assign\")\n",
        "assign_bias = graph.get_operation_by_name(\"hidden1/bias/Assign\")\n",
        "init_kernel = assign_kernel.inputs[1]\n",
        "init_bias = assign_bias.inputs[1]\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init, feed_dict={init_kernel: original_w, init_bias: original_b})\n",
        "    # [...] Train the model on your new task\n",
        "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 61.  83. 105.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vNf-UVjeGQfF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Another approach would be to create dedicated assignment nodes and dedicated placeholders. This is more verbose and less efficient, but you may find this more explicit:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3RJ2LMqFGZz7",
        "colab_type": "code",
        "outputId": "c29abaf2-5dbb-4a05-91cb-b74e6e67c853",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 2\n",
        "n_hidden1 = 3\n",
        "\n",
        "original_w = [[1., 2., 3.], [4., 5., 6.]] # Load the weights from the other framework\n",
        "original_b = [7., 8., 9.]                 # Load the biases from the other framework\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
        "# [...] Build the rest of the model\n",
        "\n",
        "# Get a handle on the variables of layer hidden1\n",
        "with tf.variable_scope(\"\", default_name=\"\", reuse=True):  # root scope\n",
        "    hidden1_weights = tf.get_variable(\"hidden1/kernel\")\n",
        "    hidden1_biases = tf.get_variable(\"hidden1/bias\")\n",
        "\n",
        "# Create dedicated placeholders and assignment nodes\n",
        "original_weights = tf.placeholder(tf.float32, shape=(n_inputs, n_hidden1))\n",
        "original_biases = tf.placeholder(tf.float32, shape=n_hidden1)\n",
        "assign_hidden1_weights = tf.assign(hidden1_weights, original_weights)\n",
        "assign_hidden1_biases = tf.assign(hidden1_biases, original_biases)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    sess.run(assign_hidden1_weights, feed_dict={original_weights: original_w})\n",
        "    sess.run(assign_hidden1_biases, feed_dict={original_biases: original_b})\n",
        "    # [...] Train the model on your new task\n",
        "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 61.  83. 105.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2TUB6h9uGb8J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note that we could also get a handle on the variables using get_collection() and specifying the scope:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ZglMYVpaGe84",
        "colab_type": "code",
        "outputId": "c07744bf-64b9-4e34-bf01-c3f45419100c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden1\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>,\n",
              " <tf.Variable 'hidden1/bias:0' shape=(3,) dtype=float32_ref>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "UbHpCZmVGgOr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Or we could use the graph's get_tensor_by_name() method:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "_oVND9PyGiPX",
        "colab_type": "code",
        "outputId": "18e7b568-6ec8-4841-b862-02b696eec5cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "gjLv8YMCGjcX",
        "colab_type": "code",
        "outputId": "37e2b79c-d03e-4ecd-9bf6-99f10264e6f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "tf.get_default_graph().get_tensor_by_name(\"hidden1/bias:0\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'hidden1/bias:0' shape=(3,) dtype=float32_ref>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "QoVtd1zxmmIZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note that we could also get a handle on the variables using get_collection() and specifying the scope:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "d8f2lhA-moHn",
        "colab_type": "code",
        "outputId": "4ce4f237-a594-4992-a1b7-c5cdce7f0877",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden1\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>,\n",
              " <tf.Variable 'hidden1/bias:0' shape=(3,) dtype=float32_ref>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "8E4AkBUUmo8I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Or we could use the graph's get_tensor_by_name() method:\n"
      ]
    },
    {
      "metadata": {
        "id": "CMmGh6JsmsND",
        "colab_type": "code",
        "outputId": "fd56c243-bf71-4c8b-c055-e5b6d4a6f18c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "AxtVrSdGmuOv",
        "colab_type": "code",
        "outputId": "32a8ecdf-1bde-4229-95da-f3aef2f21708",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "tf.get_default_graph().get_tensor_by_name(\"hidden1/bias:0\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'hidden1/bias:0' shape=(3,) dtype=float32_ref>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "YOPlzvo0mzr-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Freezing the Lower Layers\n",
        "\n",
        "It is likely that the lower layers of the first DNN have learned to detect low-level features in pictures that will be useful across both image classification tasks, so you can just reuse these layers as they are. It is generally a good idea to \"freeze\" their wights when training the new DNN: if the lower-layer weights are fixed, then the higher-layer weights will be easier to train. To freeze the lower layers during training, one solution is to give the optimizer the list of variables to train, exclusing the variables from the lower layers:  "
      ]
    },
    {
      "metadata": {
        "id": "kie966v4oJT5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300 # reused\n",
        "n_hidden2 = 50  # reused\n",
        "n_hidden3 = 50  # reused\n",
        "n_hidden4 = 20  # new!\n",
        "n_outputs = 10  # new!\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n",
        "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n",
        "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n",
        "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n",
        "\n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "edn3jk-xoTvN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"train\"):                                         \n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)     \n",
        "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"hidden[34]|outputs\")\n",
        "    training_op = optimizer.minimize(loss, var_list=train_vars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P4ZxWRWloZRi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer()\n",
        "new_saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HVDv6EQxodrf",
        "colab_type": "code",
        "outputId": "eab5b016-4ed3-4b59-e499-63801ed5241a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "cell_type": "code",
      "source": [
        "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                               scope=\"hidden[123]\") # regular expression\n",
        "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
            "0 Validation accuracy: 0.8968\n",
            "1 Validation accuracy: 0.93\n",
            "2 Validation accuracy: 0.9402\n",
            "3 Validation accuracy: 0.944\n",
            "4 Validation accuracy: 0.9478\n",
            "5 Validation accuracy: 0.9504\n",
            "6 Validation accuracy: 0.9506\n",
            "7 Validation accuracy: 0.9534\n",
            "8 Validation accuracy: 0.9556\n",
            "9 Validation accuracy: 0.9564\n",
            "10 Validation accuracy: 0.9558\n",
            "11 Validation accuracy: 0.9566\n",
            "12 Validation accuracy: 0.9564\n",
            "13 Validation accuracy: 0.9576\n",
            "14 Validation accuracy: 0.959\n",
            "15 Validation accuracy: 0.9578\n",
            "16 Validation accuracy: 0.9572\n",
            "17 Validation accuracy: 0.96\n",
            "18 Validation accuracy: 0.9588\n",
            "19 Validation accuracy: 0.9604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FMD_aBNNpOj9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Or we can call `tf.stop_gradient` on the last layer we want to freeze:"
      ]
    },
    {
      "metadata": {
        "id": "NOGvx27rofSa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300 # reused\n",
        "n_hidden2 = 50  # reused\n",
        "n_hidden3 = 50  # reused\n",
        "n_hidden4 = 20  # new!\n",
        "n_outputs = 10  # new!\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kc2B9FXcoiU2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
        "                              name=\"hidden1\") # reused frozen\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
        "                              name=\"hidden2\") # reused frozen\n",
        "    hidden2_stop = tf.stop_gradient(hidden2)\n",
        "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n",
        "                              name=\"hidden3\") # reused, not frozen\n",
        "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
        "                              name=\"hidden4\") # new!\n",
        "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hXsO8o5lokl0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gBL48V5ton3x",
        "colab_type": "code",
        "outputId": "e8c0155d-026e-46e4-d17b-eac2f024f0b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "cell_type": "code",
      "source": [
        "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                               scope=\"hidden[123]\") # regular expression\n",
        "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
            "0 Validation accuracy: 0.9018\n",
            "1 Validation accuracy: 0.931\n",
            "2 Validation accuracy: 0.9428\n",
            "3 Validation accuracy: 0.9478\n",
            "4 Validation accuracy: 0.9512\n",
            "5 Validation accuracy: 0.9522\n",
            "6 Validation accuracy: 0.9524\n",
            "7 Validation accuracy: 0.9556\n",
            "8 Validation accuracy: 0.9556\n",
            "9 Validation accuracy: 0.956\n",
            "10 Validation accuracy: 0.957\n",
            "11 Validation accuracy: 0.955\n",
            "12 Validation accuracy: 0.9574\n",
            "13 Validation accuracy: 0.9578\n",
            "14 Validation accuracy: 0.9582\n",
            "15 Validation accuracy: 0.9572\n",
            "16 Validation accuracy: 0.9564\n",
            "17 Validation accuracy: 0.9578\n",
            "18 Validation accuracy: 0.9592\n",
            "19 Validation accuracy: 0.9582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "poE4kBvmpaZG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Caching the Frozen Layers\n",
        "\n",
        "Since the frozen layers won't change, it is possible to cache the output of the topmost frozen layer for each training instance. Since training goes through the whole dataset many times, this will five you a huge speed boost as you will only need to go through the frozen layers once per training instance instead of once per epoch. For example, you could first run the whole training set through the lower layers (assuming you have enough RAM), then during training, instead of building batches of training instances, you would build batches of outputs from hidden layer 2 and feed them to the training operation:"
      ]
    },
    {
      "metadata": {
        "id": "Va5cs52Sq6F1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300 # reused\n",
        "n_hidden2 = 50  # reused\n",
        "n_hidden3 = 50  # reused\n",
        "n_hidden4 = 20  # new!\n",
        "n_outputs = 10  # new!\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
        "                              name=\"hidden1\") # reused frozen\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
        "                              name=\"hidden2\") # reused frozen & cached\n",
        "    hidden2_stop = tf.stop_gradient(hidden2)\n",
        "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n",
        "                              name=\"hidden3\") # reused, not frozen\n",
        "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
        "                              name=\"hidden4\") # new!\n",
        "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new!\n",
        "\n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "66Mnatg3rSJC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                               scope=\"hidden[123]\") # regular expression\n",
        "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U61gcwV7rT2R",
        "colab_type": "code",
        "outputId": "309dda22-ea56-494a-cc9a-13647bde702c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "n_batches = len(X_train) // batch_size\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
        "    \n",
        "    h2_cache = sess.run(hidden2, feed_dict={X: X_train})\n",
        "    h2_cache_valid = sess.run(hidden2, feed_dict={X: X_valid}) # not shown in the book\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        shuffled_idx = np.random.permutation(len(X_train))\n",
        "        hidden2_batches = np.array_split(h2_cache[shuffled_idx], n_batches)\n",
        "        y_batches = np.array_split(y_train[shuffled_idx], n_batches)\n",
        "        for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
        "            sess.run(training_op, feed_dict={hidden2:hidden2_batch, y:y_batch})\n",
        "\n",
        "        accuracy_val = accuracy.eval(feed_dict={hidden2: h2_cache_valid, # not shown\n",
        "                                                y: y_valid})             # not shown\n",
        "        print(epoch, \"Validation accuracy:\", accuracy_val)               # not shown\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
            "0 Validation accuracy: 0.9018\n",
            "1 Validation accuracy: 0.931\n",
            "2 Validation accuracy: 0.9428\n",
            "3 Validation accuracy: 0.9478\n",
            "4 Validation accuracy: 0.9512\n",
            "5 Validation accuracy: 0.9522\n",
            "6 Validation accuracy: 0.9524\n",
            "7 Validation accuracy: 0.9556\n",
            "8 Validation accuracy: 0.9556\n",
            "9 Validation accuracy: 0.956\n",
            "10 Validation accuracy: 0.957\n",
            "11 Validation accuracy: 0.955\n",
            "12 Validation accuracy: 0.9574\n",
            "13 Validation accuracy: 0.9578\n",
            "14 Validation accuracy: 0.9582\n",
            "15 Validation accuracy: 0.9572\n",
            "16 Validation accuracy: 0.9564\n",
            "17 Validation accuracy: 0.9578\n",
            "18 Validation accuracy: 0.9592\n",
            "19 Validation accuracy: 0.9582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q0cu7fk3rXEM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Faster Optimizers\n",
        "\n",
        "Applying a good initialization strategy for the connection weights, using a good activation function, using Batch Normalization, and reusing parts of a pretrained network are ways we have seen to speed up our network. Another huge speed boost comes from using a faster optimizer than the regular Gradient Descent optimzer. "
      ]
    },
    {
      "metadata": {
        "id": "AzDFzqFlw6V1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Momentum Optimizer"
      ]
    },
    {
      "metadata": {
        "id": "x5XSsNvqrdC7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KnSXJwUWy6Xj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Nesterov Accelerated Gradient"
      ]
    },
    {
      "metadata": {
        "id": "6WwcjqgJy-Pz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9, use_nesterov=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8KnSrABUznZz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### AdaGrad"
      ]
    },
    {
      "metadata": {
        "id": "t_zFRJMOzpWh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z2rtfcyuzrbA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### RMSProp"
      ]
    },
    {
      "metadata": {
        "id": "9XnZumfy3Mdx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate,momentum=0.9, decay=0.9, epsilon=1e-10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IxhMhFwZ3Mzm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Adam Optimizer"
      ]
    },
    {
      "metadata": {
        "id": "cNsx7hP43V2C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wCgROpsx3WIZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Learning Rate Scheduling"
      ]
    },
    {
      "metadata": {
        "id": "9eykcV653bGq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finding a learning rate is difficult, set it too high and it will diverge, set it too low and it will take a very long time to reach the minimum.\n",
        "\n",
        "You can train you model with different learning rates for just a few epochs and draw out the learning curves and get a good idea about what the optimal learning rate might be.\n",
        "\n",
        "However, you can do better than a constant learning rate if you start with a high learning rate and then reduce it once it stops making fast progress, you can reach a good solution faster than with the optimal constant learning rate. There are different strategies to reduce the learning rate during training. These strategies are called *learning schedules*."
      ]
    },
    {
      "metadata": {
        "id": "7Q8-5nbq4uik",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Predetermined piecewise constant learning rate\n",
        "\n",
        "For example, set the learning rate to 0.1 first then to 0.001 after 50 epochs. Although this solution can work very well, it often requires fiddling around to figure out the right learning rates and when to use them. "
      ]
    },
    {
      "metadata": {
        "id": "tB2iA9Pv7pb5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Performance scheduling\n",
        "\n",
        "Measure the validation error every N steps and reduce the learning rate by a factor of $\\lambda$ when the error stops dropping."
      ]
    },
    {
      "metadata": {
        "id": "OPcEpIGS77M3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Exponential scheduling\n",
        "Set the learning rate to a function of the iteration number $t: \\eta(t) = \\eta_{0}10^{\\frac{-t}{r}}$. This works great but it requires tuning $\\eta_{0}$ and $r$. THe learning rate will drop by a factor of 10 every $r$ steps."
      ]
    },
    {
      "metadata": {
        "id": "ZqpMG4--8tae",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Power scheduling\n",
        "\n",
        "Set the learning rate to $\\eta(t) = \\eta_{0}(1 + t/r)^{-c}$. The hyperparameter $c$ is typically set to 1. THis is similar to exponential scheduling, but the learning rate drops much slowly."
      ]
    },
    {
      "metadata": {
        "id": "9Z1LnEj79Sdv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Implementing a learning schedule in TensorFlow is pretty easy, here's an implementation using exponential scheduling with $\\eta_0=0.1\\ and\\ r=10000$: "
      ]
    },
    {
      "metadata": {
        "id": "dGZXaRNy9Rfm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 50\n",
        "n_outputs = 10\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
        "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
        "\n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l91rMf0f9kuj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"train\"):\n",
        "    initial_learning_rate = 0.1\n",
        "    decay_steps = 10000\n",
        "    decay_rate = 1/10\n",
        "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
        "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
        "                                               decay_steps, decay_rate)\n",
        "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
        "    training_op = optimizer.minimize(loss, global_step=global_step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rWfJ-buZ9oef",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u4Ez_fvz9p49",
        "colab_type": "code",
        "outputId": "9f110637-d1e5-41a8-f919-a43a99f79bb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "cell_type": "code",
      "source": [
        "n_epochs = 5\n",
        "batch_size = 50\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Validation accuracy: 0.954\n",
            "1 Validation accuracy: 0.9724\n",
            "2 Validation accuracy: 0.973\n",
            "3 Validation accuracy: 0.9788\n",
            "4 Validation accuracy: 0.9816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5GCQoJwH9skE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since AdaGrad, RMSProp, and Adam optimization automatically reduce the learning rate during training, it is not necessary to add an extra learning schedule. For other optimization algorithms, using exponential decay or performance scheduling can considerably speed up convergence."
      ]
    },
    {
      "metadata": {
        "id": "aFGFBZCkB8x0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Avoiding Overfitting Through Regularization"
      ]
    },
    {
      "metadata": {
        "id": "bzJzm-Q_CC3B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With millions of parameters in a DNN, one can overfit on the training set. Thankfully, we have some good regularization techniques for neural networks that will reduce overfitting and improve their power to generalize on the test set and beyond such as early stopping, $\\ell_1$ and $\\ell_2$ regularization, dropout, max-norm regularization, and data augmentation."
      ]
    },
    {
      "metadata": {
        "id": "XFYJtkucD8D6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Early Stopping\n",
        "\n",
        "Just interrupt the training when its performance on the validation set starts dropping. One way to implement this with TensorFlow is to evaluate the model on a validation set at regular intervals (50 steps let's say), and save a \"winner\" snapshot if it outperforms previous \"winner\" snapshots. Count the number of steps since the last \"winner\" snapshot was saved, and interrupt training when this number reaches some limit (2000) steps. Then restore the last \"winner\" snapshot.\n",
        "\n",
        "Altough early stopping works very well in practice, you can usually get much higher performance out of your network by combining it with other regulaization techniques."
      ]
    },
    {
      "metadata": {
        "id": "2x4nWAcUEwBp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### $\\ell_1$ and $\\ell_2$ regularization\n",
        "\n",
        "$\\ell_1$ and $\\ell_2$ regularizations are used to constrain the weights but not the biases usually.\n",
        "\n",
        "Let's implement $\\ell_1$ regularization manually. First, we create the model, as usual (with just one hidden layer this time, for simplicity):\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "cE9iqDKKFJhI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300\n",
        "n_outputs = 10\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
        "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BM0jn0x8F1q8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we get a handle on the layer weights, and we compute the total loss, which is equal to the sum of the usual cross entropy loss and the $\\ell_1$ loss (i.e., the absolute values of the weights):\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "L7t9Y3mTFz-0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "W1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
        "W2 = tf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n",
        "\n",
        "scale = 0.001 # l1 regularization hyperparameter\n",
        "\n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
        "    reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n",
        "    loss = tf.add(base_loss, scale * reg_losses, name=\"loss\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MY7aFkV6F8ay",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The rest is the same:"
      ]
    },
    {
      "metadata": {
        "id": "nk89VNz8F5uY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vc2i0K6RF7wg",
        "colab_type": "code",
        "outputId": "0f1ff9d4-fdcc-48dc-eb27-047bcdcb10e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "cell_type": "code",
      "source": [
        "n_epochs = 20\n",
        "batch_size = 200\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Validation accuracy: 0.831\n",
            "1 Validation accuracy: 0.871\n",
            "2 Validation accuracy: 0.8838\n",
            "3 Validation accuracy: 0.8934\n",
            "4 Validation accuracy: 0.8966\n",
            "5 Validation accuracy: 0.8988\n",
            "6 Validation accuracy: 0.9016\n",
            "7 Validation accuracy: 0.9044\n",
            "8 Validation accuracy: 0.9058\n",
            "9 Validation accuracy: 0.906\n",
            "10 Validation accuracy: 0.9068\n",
            "11 Validation accuracy: 0.9054\n",
            "12 Validation accuracy: 0.907\n",
            "13 Validation accuracy: 0.9084\n",
            "14 Validation accuracy: 0.9088\n",
            "15 Validation accuracy: 0.9064\n",
            "16 Validation accuracy: 0.9066\n",
            "17 Validation accuracy: 0.9066\n",
            "18 Validation accuracy: 0.9066\n",
            "19 Validation accuracy: 0.9052\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tC0jCDYYGAWe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A better way, if we have a lot more layers: we can pass a regularization function to the tf.layers.dense() function, which will use it to create operations that will compute the regularization loss, and it adds these operations to the collection of regularization losses. The beginning is the same as above:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "CMQl7VEjGcQ5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28  # MNIST\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 50\n",
        "n_outputs = 10\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NhPmo9EaGheF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we will use Python's partial() function to avoid repeating the same arguments over and over again. Note that we set the kernel_regularizer argument:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "bW-omgQkGdol",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "scale = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TMeeKHhFGi_x",
        "colab_type": "code",
        "outputId": "dbf1019b-67e9-432d-87b8-cbc95f1d5950",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "cell_type": "code",
      "source": [
        "my_dense_layer = partial(\n",
        "    tf.layers.dense, activation=tf.nn.relu, kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
        "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
        "    logits = my_dense_layer(hidden2, n_outputs, activation=None, name=\"outputs\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jXqzxfskHBJm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next we must add the regularization losses to the base loss:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "4zbfqOGoGoIM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"loss\"):                                     \n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)                                \n",
        "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")   \n",
        "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
        "    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9me2BY-SHHsI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And the rest is the same as usual:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "mtQPeczWHDCq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(loss)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_378cGRtHJoO",
        "colab_type": "code",
        "outputId": "e7d75fde-d9fa-4440-f2ef-ec87705134e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "cell_type": "code",
      "source": [
        "n_epochs = 20\n",
        "batch_size = 200\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Validation accuracy: 0.8274\n",
            "1 Validation accuracy: 0.8766\n",
            "2 Validation accuracy: 0.8952\n",
            "3 Validation accuracy: 0.9016\n",
            "4 Validation accuracy: 0.9082\n",
            "5 Validation accuracy: 0.9096\n",
            "6 Validation accuracy: 0.9126\n",
            "7 Validation accuracy: 0.9154\n",
            "8 Validation accuracy: 0.9178\n",
            "9 Validation accuracy: 0.919\n",
            "10 Validation accuracy: 0.92\n",
            "11 Validation accuracy: 0.9224\n",
            "12 Validation accuracy: 0.9212\n",
            "13 Validation accuracy: 0.9228\n",
            "14 Validation accuracy: 0.9224\n",
            "15 Validation accuracy: 0.9216\n",
            "16 Validation accuracy: 0.9218\n",
            "17 Validation accuracy: 0.9228\n",
            "18 Validation accuracy: 0.9216\n",
            "19 Validation accuracy: 0.9214\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XvSMdeOlHLYp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Dropout\n",
        "\n",
        "The most popular regularization technique for deep neural netowrks is arguably *dropout*.\n",
        "\n",
        "It is a fairly simple algorithm : at every training step, every neuron (including the input neurons but excluding the output neurons) has a probability *p* of being temporarily \"dropped out\", meaning it will be entirely ignored during this training step, but it may be active during the next step. The hyperparameter *p* is called the *dropout rate*, and it is typically set to 50%. After training, neurons don't get dropped anymore.\n",
        "\n",
        "There is one important technical detail to consider here and it is that at testing a neuron is connected to more input neurons than it will be during training. To compensate for this, we multiply the weights by $(1-p)$, called the keep probability, after training or we could divide the weights by the keep probability during training and this is what we will implement in TensorFlow."
      ]
    },
    {
      "metadata": {
        "id": "Bz9S35H0DJl9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rfpGGJZCDqpo",
        "colab_type": "code",
        "outputId": "80b316ce-49ae-4002-ebe9-00468f8bf6d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "cell_type": "code",
      "source": [
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "\n",
        "dropout_rate = 0.5  # == 1 - keep_prob\n",
        "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
        "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
        "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
        "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
        "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-68-2eb91232f5ac>:5: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uVYzYO79Ds3i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
        "    training_op = optimizer.minimize(loss)    \n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "    \n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K0CEKvwgD6IX",
        "colab_type": "code",
        "outputId": "42c4456d-7857-4aba-deaf-69a9a963bca8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "cell_type": "code",
      "source": [
        "n_epochs = 20\n",
        "batch_size = 50\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
        "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
        "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
        "\n",
        "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Validation accuracy: 0.9264\n",
            "1 Validation accuracy: 0.9464\n",
            "2 Validation accuracy: 0.9518\n",
            "3 Validation accuracy: 0.9554\n",
            "4 Validation accuracy: 0.9592\n",
            "5 Validation accuracy: 0.963\n",
            "6 Validation accuracy: 0.9618\n",
            "7 Validation accuracy: 0.965\n",
            "8 Validation accuracy: 0.971\n",
            "9 Validation accuracy: 0.9686\n",
            "10 Validation accuracy: 0.9706\n",
            "11 Validation accuracy: 0.9714\n",
            "12 Validation accuracy: 0.97\n",
            "13 Validation accuracy: 0.9706\n",
            "14 Validation accuracy: 0.9744\n",
            "15 Validation accuracy: 0.9704\n",
            "16 Validation accuracy: 0.9712\n",
            "17 Validation accuracy: 0.974\n",
            "18 Validation accuracy: 0.9734\n",
            "19 Validation accuracy: 0.9744\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wWUFRhdeIGRO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Max Norm Regularization\n",
        "\n",
        "Another regularization technique that is quite popular for neural networks is called *max-norm regularization:* for each neuron, it constrains the weights $\\boldsymbol{w}$ of the incoming connections such that $\\left\\lVert\\boldsymbol{w}\\right\\rVert_2 \\leq r, where\\ r\\ is\\ the\\ max-norm\\ hyperparameter\\ and\\ \\left\\lVert \\cdot \\right\\rVert_2 is\\ the\\ \\ell_2\\ norm$.\n",
        "\n",
        "We typically implement this constraint by computing $\\left\\lVert\\boldsymbol{w}\\right\\rVert_2$ after each training step and clipping $\\boldsymbol{w}$ if needed: $\\boldsymbol{w} \\leftarrow \\boldsymbol{w}\\frac{r}{\\left\\lVert\\boldsymbol{w}\\right\\rVert_2}$.\n",
        "\n",
        "Reducing $r$ increases the amount of regularization and helps reduce overfitting. Max-norm regularization can also help alleviate the vanishing/exploding gradients problems (if you are not using Batch Normalization).\n",
        "\n",
        "TensorFlow does not provide an off-the-shelf max-norm regularizer, but it is not too hard to implement.. The following code gets handle on the weights of the first hidden layer, then it uses the `clip_by_norm()` function to create an operation that will clip the weights along the second axis so that each row vector ends up with a maximum norm of 1.0. The last line creates an assignment operation that will assign the clipped weights to the weights variable."
      ]
    },
    {
      "metadata": {
        "id": "fNE1BLQjD874",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's go back to a plain and simple neural net for MNIST with just 2 hidden layers:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "08Bcm6WNEHXs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 50\n",
        "n_outputs = 10\n",
        "\n",
        "learning_rate = 0.01\n",
        "momentum = 0.9\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
        "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
        "\n",
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
        "    training_op = optimizer.minimize(loss)    \n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y9M5n5EGMXpV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, let's get a handle on the first hidden layer's weight and create an operation that will compute the clipped weights using the clip_by_norm() function. Then we create an assignment operation to assign the clipped weights to the weights variable:"
      ]
    },
    {
      "metadata": {
        "id": "9W2YgxGPMUnU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "threshold = 1.0\n",
        "weights = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
        "clipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\n",
        "clip_weights = tf.assign(weights, clipped_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lLv-LaDvMc-P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can do this as well for the second hidden layer:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "OfKJ88V5Ma9l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weights2 = tf.get_default_graph().get_tensor_by_name(\"hidden2/kernel:0\")\n",
        "clipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\n",
        "clip_weights2 = tf.assign(weights2, clipped_weights2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s7PlfWlrMfNJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's add an initializer and a saver:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "cdPFfXVTMhms",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xgPBcyKGMlWS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And now we can train the model. It's pretty much as usual, except that right after running the training_op, we run the clip_weights and clip_weights2 operations:"
      ]
    },
    {
      "metadata": {
        "id": "sB3cMCSlMjfW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_epochs = 20\n",
        "batch_size = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wTOgnikRMo1R",
        "colab_type": "code",
        "outputId": "7e19f227-626e-4c0a-cf0f-39fe034d2279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:                                              \n",
        "    init.run()                                                          \n",
        "    for epoch in range(n_epochs):                                       \n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): \n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "            clip_weights.eval()\n",
        "            clip_weights2.eval()                                        \n",
        "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})   \n",
        "        print(epoch, \"Validation accuracy:\", acc_valid)                 \n",
        "\n",
        "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Validation accuracy: 0.9568\n",
            "1 Validation accuracy: 0.9696\n",
            "2 Validation accuracy: 0.9716\n",
            "3 Validation accuracy: 0.9772\n",
            "4 Validation accuracy: 0.9772\n",
            "5 Validation accuracy: 0.9774\n",
            "6 Validation accuracy: 0.9822\n",
            "7 Validation accuracy: 0.981\n",
            "8 Validation accuracy: 0.98\n",
            "9 Validation accuracy: 0.9824\n",
            "10 Validation accuracy: 0.9822\n",
            "11 Validation accuracy: 0.9852\n",
            "12 Validation accuracy: 0.9824\n",
            "13 Validation accuracy: 0.984\n",
            "14 Validation accuracy: 0.9842\n",
            "15 Validation accuracy: 0.9842\n",
            "16 Validation accuracy: 0.984\n",
            "17 Validation accuracy: 0.9834\n",
            "18 Validation accuracy: 0.9842\n",
            "19 Validation accuracy: 0.9844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3-0FGhC3NGpA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The implementation above is straightforward and it works fine, but it is a bit messy. A better approach is to define a max_norm_regularizer() function:"
      ]
    },
    {
      "metadata": {
        "id": "G1QmA9XtMxXF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def max_norm_regularizer(threshold, axes=1, name=\"max_norm\", collection=\"max_norm\"):\n",
        "    def max_norm(weights):\n",
        "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
        "        clip_weights = tf.assign(weights, clipped, name=name)\n",
        "        tf.add_to_collection(collection, clip_weights)\n",
        "        return None # there is no regularization loss term\n",
        "    return max_norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3EonmgEhNcqp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then we can call this function to get a max norm regularizer (with the threshold you want). When we create a hidden layer, we can pass this regularizer to the kernel_regularizer argument:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "WW_Wj5IaNXo0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_graph()\n",
        "\n",
        "n_inputs = 28 * 28\n",
        "n_hidden1 = 300\n",
        "n_hidden2 = 50\n",
        "n_outputs = 10\n",
        "\n",
        "learning_rate = 0.01\n",
        "momentum = 0.9\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
        "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dy5ATnaGNsrx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_norm_reg = max_norm_regularizer(threshold=1.0)\n",
        "\n",
        "with tf.name_scope(\"dnn\"):\n",
        "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, kernel_regularizer=max_norm_reg, name=\"hidden1\")\n",
        "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, kernel_regularizer=max_norm_reg, name=\"hidden2\")\n",
        "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1EAdIqpGNull",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.name_scope(\"loss\"):\n",
        "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
        "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "\n",
        "with tf.name_scope(\"train\"):\n",
        "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
        "    training_op = optimizer.minimize(loss)    \n",
        "\n",
        "with tf.name_scope(\"eval\"):\n",
        "    correct = tf.nn.in_top_k(logits, y, 1)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JDYd-wGBOV6Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Training is as usual, except you must run the weights clipping operations after each training operation:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "kDz51dfmOUGS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_epochs = 20\n",
        "batch_size = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xn0FigffOXsC",
        "colab_type": "code",
        "outputId": "81c1a265-5187-4d48-d5f8-d52cc32f8f53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "cell_type": "code",
      "source": [
        "clip_all_weights = tf.get_collection(\"max_norm\")\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    for epoch in range(n_epochs):\n",
        "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
        "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
        "            sess.run(clip_all_weights)\n",
        "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid}) \n",
        "        print(epoch, \"Validation accuracy:\", acc_valid)               \n",
        "\n",
        "    save_path = saver.save(sess, \"./my_model_final.ckpt\")             "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Validation accuracy: 0.9562\n",
            "1 Validation accuracy: 0.971\n",
            "2 Validation accuracy: 0.9728\n",
            "3 Validation accuracy: 0.9752\n",
            "4 Validation accuracy: 0.9752\n",
            "5 Validation accuracy: 0.9772\n",
            "6 Validation accuracy: 0.9798\n",
            "7 Validation accuracy: 0.9798\n",
            "8 Validation accuracy: 0.9814\n",
            "9 Validation accuracy: 0.9808\n",
            "10 Validation accuracy: 0.9824\n",
            "11 Validation accuracy: 0.9824\n",
            "12 Validation accuracy: 0.9806\n",
            "13 Validation accuracy: 0.9818\n",
            "14 Validation accuracy: 0.9822\n",
            "15 Validation accuracy: 0.9808\n",
            "16 Validation accuracy: 0.9816\n",
            "17 Validation accuracy: 0.982\n",
            "18 Validation accuracy: 0.9812\n",
            "19 Validation accuracy: 0.9816\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}